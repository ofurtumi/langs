{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "Upphaflega f√≥r skr√∂pun fram √≠ √æessu skjali en √°kva√∞ a√∞ splitta √æv√≠ upp svo h√¶gt v√¶ri a√∞ keyra alla gpt-√æj√°lfunina √° collab √°n √æess a√∞ √æurfa keyra allt drasli√∞ aftur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install tokenizers\n",
    "%pip install transformers\n",
    "%pip install seqeval\n",
    "%pip install accelerate==0.24.1\n",
    "%pip install wandb\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 √æj√°lfun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form√∂ttum g√∂gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 3332\n",
      "Test dataset length: 588\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# with open('/content/drive/MyDrive/scraped_data.json') as f:\n",
    "with open('scraping/scraped_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "lyrics = list(data.values())\n",
    "for i in range(len(lyrics)):\n",
    "    lyrics[i] = \"\\n\".join(lyrics[i])\n",
    "\n",
    "def build_text_files(data_json, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    for texts in data_json:\n",
    "        data += texts + \"  \"\n",
    "    f.write(data)\n",
    "\n",
    "train, test = train_test_split(lyrics,test_size=0.15)\n",
    "\n",
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')\n",
    "\n",
    "print(\"Train dataset length: \"+str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hl√∂√∞um inn t√≥ka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jonfd/gpt2-igc-is\")\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerum g√∂gnin \"√æj√°lfanleg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (118490 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stillum √æj√°lfarann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"jonfd/gpt2-igc-is\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-textar\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=5, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √ûj√°lfum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vistum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"gpt2-textar\")\n",
    "trainer.push_to_hub(\"ofurtumi/gpt2textar_v0.1\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 908/908 [00:00<00:00, 1.02MB/s]\n",
      "Downloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500M/500M [01:04<00:00, 7.81MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gen1 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.1\", tokenizer=\"jonfd/gpt2-igc-is\", pad_token_id=50256)\n",
    "gen2 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.2\", tokenizer=\"jonfd/gpt2-igc-is\", pad_token_id=50256)\n",
    "gen3 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.3\", tokenizer=\"jonfd/gpt2-igc-is\", pad_token_id=50256)\n",
    "\n",
    "versions = {\"v0.1\": gen1, \"v0.2\": gen2, \"v0.3\": gen3}\n",
    "# gen03 = pipeline(\"text-generation\", model=\"ofurtumi/gpt2textar_v0.1\", tokenizer=\"jonfd/gpt2-igc-is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_versions(prompt):\n",
    "    for name, version in versions.items():\n",
    "        print(name)\n",
    "        print(version(prompt, do_sample=True, temperature=0.9)[0][\"generated_text\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "Kisuv√¶l um landi√∞,\n",
      "og um heiminn √©g √¶tla ekki a√∞ flj√∫ga.\n",
      "√ûv√≠ √æ√¶r eru svo miki√∞ fj√∂r.\n",
      "L√°ta √æ√¶r sl√° √≠ sig endalaust,\n",
      "alla daga sem kv√∂ld.  Lengi getur ma√∞ur veri√∞ fur√∞ufugl,\n",
      "\n",
      "v0.2\n",
      "Kisuv√¶l\n",
      "En gamla f√≥lki√∞ √æa√∞ segir:\n",
      "Ja, ja, h√∫n er n√∫ aldeilis angr√≠\n",
      "og √æessi lambi h√∫n St√≠na,\n",
      "og √æessi lambi er n√∫ h√©r,\n",
      "n√∫ er ekki um √æa√∞ neitt neitt a√∞ r√¶√∞a\n",
      "\n",
      "v0.3\n",
      "Kisuv√¶l og l√¶ti\n",
      "Hettud√∫fa √∫t √° t√∫ni,\n",
      "h√¶, hann sag√∞i ei neitt.\n",
      "√ìli skans og Sn√¶finnur snj√≥karl\n",
      "kisa √∫t √≠ m√≥a,\n",
      "kisa hlj√≥p √≠ √°tt til m√≠n\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"Kisuv√¶l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "F√≥lk sem hefur alltof gaman\n",
      "kann ekkert um l√≠fi√∞ a√∞ draga.\n",
      "√âg √° von √° √æ√©r ein,\n",
      "√æ√∫ ert fyrsta vonarstjarna m√≠n.\n",
      "√âg var √°√∞ur eitt, en er kominn til a√∞ vera.\n",
      "√âg elska √æig eins\n",
      "\n",
      "v0.2\n",
      "F√≥lk sem hefur alltof gaman\n",
      "en samt er √æa√∞ bara hali\n",
      "a√∞ hafa √æa√∞ k√≥tilettu me√∞ rau√∞korg er √æa√∞ sem √©g hef\n",
      "√ç sumar √¶tla √©g a√∞ skemmta m√©r\n",
      "og hafa gaman af\n",
      "√©g veit samt ekki nema √©g\n",
      "\n",
      "v0.3\n",
      "F√≥lk sem hefur alltof gaman.\n",
      "Sorr√≠ me√∞ mig, sorr√≠ me√∞ mig, sorr√≠ me√∞ mig, sorr√≠ me√∞ mig, sorr√≠ me√∞ mig, sorr√≠ me√∞ mig.\n",
      "√âg elska √æig og √æ√∫ elskar mig,\n",
      "√æ√∫\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"F√≥lk sem hefur alltof gaman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.1\n",
      "Texti um barn sem t√Ωndi boltanum s√≠num\n",
      "Og sem var √æa√∞ sem √æau voru ekki\n",
      "√âg s√° √æennan dreng sem spila√∞i me√∞ √çA\n",
      "Vi√∞ g√¶tum haft √æa√∞ svo fr√°b√¶rt a√∞ koma √æ√©r ekki fyrir kattarnef\n",
      "en √æ√° v√¶rum vi√∞ bara vitlaus\n",
      "Og\n",
      "\n",
      "v0.2\n",
      "Texti um barn sem t√Ωndi boltanum s√≠num\n",
      "√Åttum okkur augnablik\n",
      "√ç l√≠fi sem er eitt augnablik\n",
      "J√° √©g √° m√©r draum sem er a√∞ r√¶tast\n",
      "√âg finn mig alveg heilan og hress\n",
      "√âg √° m√©r draum sem er a√∞ r√¶tast\n",
      "√âg √°\n",
      "\n",
      "v0.3\n",
      "Texti um barn sem t√Ωndi boltanum s√≠num\n",
      "Vi√∞ gleymum √∂llu\n",
      "og hva√∞ √æa√∞ var sem var\n",
      "Hva√∞ √æa√∞ var sem var\n",
      "Hva√∞ √æa√∞ var sem var\n",
      "Allt sem vi√∞ hugsu√∞um √≠ g√¶r,\n",
      "hva√∞ √æa√∞ var sem var\n",
      "Hva√∞ √æa√∞ var\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_versions(\"Texti um barn sem t√Ωndi boltanum s√≠num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
